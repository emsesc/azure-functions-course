## Checkpoint 2

> Time to test our completed Azure Function! It should now successfully do these tasks:

* Parse the image
* Send image to Face API and return Face data

This time, we won't need to add any additional code, as the completed function should return the emotion data on its own.

<details>
<summary>These are the same steps as Checkpoint #1. Click here if you need them:</summary>
<br>

* Navigate back to the Postman Chrome extension app and change GET to POST

![Untitled_ Nov 11, 2020 6_24 PM](https://user-images.githubusercontent.com/69332964/98876201-c3bca780-244b-11eb-9b94-8d3cecc80115.gif)

* Copy your function's url from the Azure Function App portal like this:

![httptrigger - Microsoft Azure](https://user-images.githubusercontent.com/69332964/98876502-6f65f780-244c-11eb-832b-a25888b980da.gif)

* Use the function url and any image you want to send the POST request. Remember to attach the file in Body!

![Untitled_ Nov 11, 2020 6_40 PM](https://user-images.githubusercontent.com/69332964/98876997-780afd80-244d-11eb-87fc-13822d909f2f.gif)

</details>

Only difference is that we should *get an output in Postman instead:
